{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc55b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/12 21:45:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nche').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2702ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_full = spark.read.csv('insurance_dataset.csv', header=True)\n",
    "df_infor = spark.read.csv('information.csv', header=True)\n",
    "df_srw = spark.read.csv('insurance_dataset - srw.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718aded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_srw = df_srw.drop('exercise_frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca8534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, isnan, when, count\n",
    "#Convert the string type varibale into integer or float\n",
    "df_infor = df_infor.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "df_infor = df_infor.withColumn(\"children\", col(\"children\").cast(\"int\"))\n",
    "df_infor = df_infor.withColumn(\"charges\", col(\"charges\").cast(\"int\"))\n",
    "df_srw = df_srw.withColumn(\"bmi\", col(\"bmi\").cast(\"int\"))\n",
    "df_srw = df_srw.withColumn(\"charges\", col(\"charges\").cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6421a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_srw = df_srw.na.drop('any')\n",
    "df_infor = df_infor.na.drop('any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde07761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#integrate two data set\n",
    "df_infor = df_infor.drop('charges')\n",
    "insurance_df = df_infor.join(df_srw,['ID'], how='inner').drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2239576",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def create_new_feature(bmi, smoker, medical_history):\n",
    "    if bmi < 25 and smoker == 'no' and medical_history == 'None':\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "# Register the UDF with PySpark\n",
    "create_new_feature_udf = udf(create_new_feature, StringType())\n",
    "\n",
    "# Apply the UDF to create the new column\n",
    "merged_df = insurance_df.withColumn('Risk_Level', create_new_feature_udf(insurance_df['bmi'], insurance_df['smoker'], insurance_df['medical_history']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c96860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "value_counts = merged_df.groupBy('Risk_Level').count().orderBy('Risk_Level')\n",
    "result = value_counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d237399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#convert the data from characters into number\n",
    "gender = {'female':1,'male':0}\n",
    "smoker = {'yes':1, 'no':0}\n",
    "region = {'southeast':0, 'northwest':1, 'southwest':2, 'northeast':3}\n",
    "medical = {'Diabetes':1, 'None':0, 'High blood pressure':2, 'Heart disease':3}\n",
    "occupation = {'Blue collar':0, 'Unemployed':1, 'Student':2, 'White collar':3}\n",
    "coverage_level = {'Basic':0, 'Standard':1,'Premium':2}\n",
    "\n",
    "data = merged_df.rdd.map(lambda x: (gender[x.gender], region[x.region], occupation[x.occupation], smoker[x.smoker],\\\n",
    "                                   medical[x.medical_history], medical[x.family_medical_history],\\\n",
    "                                    coverage_level[x.coverage_level], x.age, x.bmi, x.children, x.Risk_Level, x.charges)).toDF(['gender', 'region','occupation', 'smoker', 'medical_history',\\\n",
    "                                                                          'family_medical_history', 'coverage_level','age','bmi', 'children', 'Risk_Level','charges'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dfea89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='gender', outputCol='gender_I')\n",
    "data = indexer.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e186d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = data.withColumn(\"Risk_Level\", col(\"Risk_Level\").cast(\"int\"))\n",
    "#result_df.dtypes\n",
    "\n",
    "result_df.columns\n",
    "r_df = result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b0f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selection\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "cols_name = ['gender_I','region','occupation','smoker','medical_history','family_medical_history','coverage_level','Risk_Level',\\\n",
    "                   'age', 'bmi', 'children']\n",
    "\n",
    "# Assemble the feature vector\n",
    "assembler = VectorAssembler(inputCols=cols_name, outputCol=\"features\")\n",
    "assembled_df = assembler.transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ced27f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/12 21:46:01 WARN Instrumentation: [0106d5cc] regParam is zero, which might cause numerical instability and overfitting.\n",
      "23/10/12 21:46:03 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/10/12 21:46:03 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/10/12 21:46:07 WARN InstanceBuilder$NativeLAPACK: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lasso = LinearRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol='charges', \n",
    "    elasticNetParam=1.0,       # This sets L1 regularization \n",
    ")\n",
    "\n",
    "lasso_model = lasso.fit(assembled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "460477e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[gender_I: double, region: bigint, occupation: bigint, smoker: bigint, medical_history: bigint, family_medical_history: bigint, coverage_level: bigint, age: bigint, bmi: bigint, children: bigint]\n"
     ]
    }
   ],
   "source": [
    "coefficients = lasso_model.coefficients\n",
    "threshold = 0.0  # Set your own threshold\n",
    "selected_feature_columns = [feature for i, feature in enumerate(cols_name) if coefficients[i] > threshold]\n",
    "selected_df = assembled_df.select(selected_feature_columns)\n",
    "\n",
    "print(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d4401f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Risk_Level|count|\n",
      "+----------+-----+\n",
      "|         1| 3166|\n",
      "|         0| 3230|\n",
      "+----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances in balanced dataset: 6396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 28:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#balance the model\n",
    "#input_features = result_df.drop('charges')\n",
    "input_features = result_df\n",
    "majority_class = input_features.filter(col(\"Risk_Level\") == 0)\n",
    "minority_class = input_features.filter(col(\"Risk_Level\") == 1)\n",
    "\n",
    "# Resample the majority class to match the minority class\n",
    "majority_count = majority_class.count()\n",
    "minority_count = minority_class.count()\n",
    "\n",
    "# Calculate the ratio for resampling\n",
    "resampling_ratio = minority_count / majority_count\n",
    "\n",
    "majority_downsampled = majority_class.sample(withReplacement=False, fraction=resampling_ratio, seed=42)\n",
    "\n",
    "# Combine the resampled majority class with the minority class\n",
    "balanced_df = minority_class.union(majority_downsampled)\n",
    "\n",
    "# Verify the balance\n",
    "balanced_category_counts = balanced_df.groupBy(\"Risk_Level\").count()\n",
    "\n",
    "# Show the counts for each category\n",
    "balanced_category_counts.show()\n",
    "\n",
    "print(\"Total instances in balanced dataset:\", balanced_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fea263d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+------+---------------+----------------------+--------------+---+---+--------+----------+-------+--------+----------------+\n",
      "|gender|region|occupation|smoker|medical_history|family_medical_history|coverage_level|age|bmi|children|Risk_Level|charges|gender_I|charges_category|\n",
      "+------+------+----------+------+---------------+----------------------+--------------+---+---+--------+----------+-------+--------+----------------+\n",
      "|     0|     2|         3|     0|              0|                     0|             2| 37| 23|       1|         1|  13782|     1.0|               2|\n",
      "|     1|     2|         1|     0|              0|                     0|             2| 21| 22|       0|         1|   8904|     0.0|               2|\n",
      "|     1|     3|         3|     0|              0|                     3|             2| 21| 22|       2|         1|  16560|     0.0|               2|\n",
      "|     1|     3|         3|     0|              0|                     3|             1| 58| 19|       0|         1|  15768|     0.0|               2|\n",
      "|     1|     1|         1|     0|              0|                     3|             0| 42| 23|       5|         1|   9889|     0.0|               2|\n",
      "|     0|     2|         3|     0|              0|                     2|             0| 41| 21|       4|         1|  11084|     1.0|               2|\n",
      "|     0|     2|         1|     0|              0|                     1|             0| 23| 21|       1|         1|   6930|     1.0|               2|\n",
      "|     1|     3|         3|     0|              0|                     1|             1| 25| 19|       2|         1|  10961|     0.0|               2|\n",
      "|     1|     3|         0|     0|              0|                     0|             2| 43| 24|       3|         1|  12238|     0.0|               2|\n",
      "|     1|     0|         2|     0|              0|                     2|             0| 34| 21|       4|         1|   7075|     0.0|               2|\n",
      "|     0|     2|         2|     0|              0|                     2|             2| 55| 20|       2|         1|  12329|     1.0|               2|\n",
      "|     0|     3|         3|     0|              0|                     1|             0| 59| 21|       5|         1|  11548|     1.0|               2|\n",
      "|     1|     3|         2|     0|              0|                     1|             0| 54| 22|       3|         1|   8227|     0.0|               2|\n",
      "|     1|     1|         1|     0|              0|                     0|             1| 20| 20|       1|         1|   5808|     0.0|               2|\n",
      "|     1|     1|         1|     0|              0|                     2|             0| 56| 18|       0|         1|   6595|     0.0|               2|\n",
      "|     0|     1|         2|     0|              0|                     2|             1| 61| 20|       3|         1|   9248|     1.0|               2|\n",
      "|     1|     2|         2|     0|              0|                     1|             1| 21| 24|       3|         1|   9168|     0.0|               2|\n",
      "|     1|     2|         3|     0|              0|                     3|             2| 53| 18|       1|         1|  18420|     0.0|               2|\n",
      "|     1|     3|         2|     0|              0|                     2|             2| 38| 21|       3|         1|  11923|     0.0|               2|\n",
      "|     0|     2|         3|     0|              0|                     0|             0| 51| 19|       0|         1|   7388|     1.0|               2|\n",
      "+------+------+----------+------+---------------+----------------------+--------------+---+---+--------+----------+-------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "conditions = [\n",
    "    (balanced_df['charges'] >= 22856),\n",
    "    (balanced_df['charges'] < 22856)\n",
    "]\n",
    "\n",
    "# Define the corresponding categories\n",
    "categories = [1, 2]\n",
    "\n",
    "# Add a new column \"charges_category\" based on the conditions\n",
    "balanced_df = balanced_df.withColumn(\n",
    "    \"charges_category\",\n",
    "    when(conditions[0], categories[0])\n",
    "    .when(conditions[1], categories[1])\n",
    "    .otherwise(None)\n",
    ")\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "balanced_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "976dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5927579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random forest ensemble has an accuracy of: 96.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 49:===========================================>              (3 + 1) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "  inputCols=['gender',\n",
    " 'region',\n",
    " 'occupation',\n",
    " 'smoker',\n",
    " 'medical_history',\n",
    " 'family_medical_history',\n",
    " 'coverage_level',\n",
    " 'age',\n",
    " 'bmi',\n",
    " 'children',\n",
    " 'Risk_Level',\n",
    " 'gender_I'],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(balanced_df)\n",
    "train_data,test_data = output.randomSplit([0.8,0.2])\n",
    "rfc = RandomForestClassifier(labelCol='charges_category',featuresCol='features')\n",
    "rfc_model = rfc.fit(train_data)\n",
    "rfc_predictions = rfc_model.transform(test_data)\n",
    "# Let's import the evaluator.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error. \n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"charges_category\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rfc_acc = acc_evaluator.evaluate(rfc_predictions)\n",
    "print('A random forest ensemble has an accuracy of: {0:2.2f}%'.format(rfc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3aceee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e2b69d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
